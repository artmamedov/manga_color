{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ghhlnxZ7sosA"
   },
   "outputs": [],
   "source": [
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.vision.gan import *\n",
    "from fastai.imports import *\n",
    "from PIL import Image\n",
    "from torchvision.models import vgg16_bn\n",
    "from torchvision import transforms\n",
    "from ipywidgets import FileUpload\n",
    "from pathlib import Path\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\".\") #Adjust path accordingly\n",
    "path_color = path/'c'\n",
    "path_bw = path/'b'\n",
    "path_model = path_bw/'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_ID = \"1wzPVTOTluuaRAqANHmw-fEVdd6FJQWTO\"\n",
    "MODEL_URL = f\"https://drive.google.com/uc?export=download&id={FILE_ID}\"\n",
    "gdown.download(MODEL_URL, \"gan256-2c.pth\",quiet = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"gan256-2c.pth\").rename(path_model/\"gan256-2c.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, size= 4 ,256 #Batch size, size of images\n",
    "arch = models.resnet18#Architecture with only 18 layers because running out of memory\n",
    "src = ImageImageList.from_folder(path_bw).split_by_rand_pct(0.1, seed=42) #Testing data, 10% validation\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(bs,size, num_workers = 0):\n",
    "    #label_from_func means that the correct label is going to be the colored image from the color folder\n",
    "    #Transforms: Zoom, horizontal flip, need more?\n",
    "    data = (src.label_from_func(lambda x: path_color/x.name)\n",
    "           .transform(get_transforms(max_zoom=1.2, do_flip = True), size=size, tfm_y=True)\n",
    "           .databunch(bs=bs,num_workers = num_workers).normalize(imagenet_stats, do_y=True))\n",
    "\n",
    "    data.c = 3 #Channels: Because we need 3 outputs in final layer: R, G, and B\n",
    "    return data\n",
    "\n",
    "def get_crit_data(classes, bs, size):\n",
    "    src = ImageList.from_folder(path, include=classes).split_by_rand_pct(0.1, seed=42)\n",
    "    ll = src.label_from_folder(classes=classes)\n",
    "    data = (ll.transform(get_transforms(max_zoom=1.2), size=size)\n",
    "           .databunch(bs=bs).normalize(imagenet_stats))\n",
    "    data.c = 3\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_gen = 'image_gen'\n",
    "data = get_data(bs, size, num_workers)\n",
    "data_crit = get_crit_data([name_gen, 'color_cropped'], bs=bs, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a pretrained model\n",
    "vgg_m = vgg16_bn(True).features.cuda().eval()\n",
    "requires_grad(vgg_m, False)\n",
    "#For feature loss\n",
    "blocks = [i-1 for i,o in enumerate(children(vgg_m)) if isinstance(o,nn.MaxPool2d)]\n",
    "blocks, [vgg_m[i] for i in blocks]\n",
    "\n",
    "def gram_matrix(x):\n",
    "    n,c,h,w = x.size()\n",
    "    x = x.view(n, c, -1)\n",
    "    return (x @ x.transpose(1,2))/(c*h*w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureLoss(nn.Module):\n",
    "    def __init__(self, m_feat, layer_ids, layer_wgts):\n",
    "        super().__init__()\n",
    "        self.m_feat = m_feat\n",
    "        self.loss_features = [self.m_feat[i] for i in layer_ids]\n",
    "        self.hooks = hook_outputs(self.loss_features, detach=False)\n",
    "        self.wgts = layer_wgts\n",
    "        self.metric_names = ['pixel',] + [f'feat_{i}' for i in range(len(layer_ids))\n",
    "              ] + [f'gram_{i}' for i in range(len(layer_ids))]\n",
    "\n",
    "    def make_features(self, x, clone=False):\n",
    "        self.m_feat(x)\n",
    "        return [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        out_feat = self.make_features(target, clone=True)\n",
    "        in_feat = self.make_features(input)\n",
    "        self.feat_losses = [base_loss(input,target)]\n",
    "        self.feat_losses += [base_loss(f_in, f_out)*w\n",
    "                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n",
    "        self.feat_losses += [base_loss(gram_matrix(f_in), gram_matrix(f_out))*w**2 * 5e3\n",
    "                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n",
    "        self.metrics = dict(zip(self.metric_names, self.feat_losses))\n",
    "        return sum(self.feat_losses)\n",
    "    \n",
    "    def __del__(self): self.hooks.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_loss = FeatureLoss(vgg_m, blocks[2:5], [5,15,2])\n",
    "loss_critic = AdaptiveLoss(nn.BCEWithLogitsLoss())\n",
    "base_loss = F.l1_loss #loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = 1e-3\n",
    "def create_gen_learner():\n",
    "    return unet_learner(data, arch, wd=wd, loss_func=feat_loss, callback_fns=LossMetrics,\n",
    "                     blur=False, norm_type=NormType.Weight)\n",
    "def create_critic_learner(data, metrics):\n",
    "    return Learner(data, gan_critic(), metrics=metrics, loss_func=loss_critic, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_gen = create_gen_learner()\n",
    "learn_crit = create_critic_learner(data_crit, metrics=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "switcher = partial(AdaptiveGANSwitcher, critic_thresh=0.75)\n",
    "learn = GANLearner.from_learners(learn_gen, learn_crit, weights_gen=(1.,50.), show_img=False, switcher=switcher,\n",
    "                                 opt_func=partial(optim.Adam, betas=(0.,0.99)), wd=wd)\n",
    "learn.callback_fns.append(partial(GANDiscriminativeLR, mult_lr=5.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concat_h(images):\n",
    "    widths, heights = zip(*(i.size for i in images))\n",
    "    total_width = sum(widths)\n",
    "    max_height = max(heights)\n",
    "    \n",
    "    output = Image.new('RGB', (total_width, max_height))\n",
    "\n",
    "    x_offset = 0\n",
    "    for im in images:\n",
    "        output.paste(im, (x_offset,0))\n",
    "        x_offset += im.size[0]\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_concat_v(images):\n",
    "    widths, heights = zip(*(i.size for i in images))\n",
    "    total_width = max(widths)\n",
    "    max_height = sum(heights)\n",
    "    \n",
    "    output = Image.new('RGB', (total_width, max_height))\n",
    "\n",
    "    y_offset = 0\n",
    "    for im in images:\n",
    "        output.paste(im, (0, y_offset))\n",
    "        y_offset += im.size[1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_stitch(images):\n",
    "    images_hr = []\n",
    "    for img in images:\n",
    "        try:\n",
    "            img = pil2tensor(img,np.float32).div(255)\n",
    "            p,img_hr,b = learn_gen.predict(fastai.vision.image.Image(img))\n",
    "        except:\n",
    "            p,img_hr,b = learn_gen.predict(img)\n",
    "\n",
    "        img_clamped = torch.clamp(img_hr, 0, 1)\n",
    "        images_hr.append(transforms.ToPILImage(mode=\"RGB\")(img_clamped))\n",
    "    combined = get_concat_h(images_hr)\n",
    "    return combined\n",
    "\n",
    "def crop_list(im, height, width):\n",
    "    imgwidth, imgheight = im.size\n",
    "    cropped_list = []\n",
    "    for i in range(0,imgheight,height):\n",
    "        for j in range(0,imgwidth,width):\n",
    "            box = (j, i, j+width, i+height)\n",
    "            output = im.crop(box)\n",
    "            cropped_list.append(output)\n",
    "            \n",
    "    return cropped_list, imgwidth, imgheight\n",
    "\n",
    "def color_image(img, width = 512, height = 512):\n",
    "    cropped_list, imgwidth, imgheight = crop_list(img, width, height)\n",
    "    combined_list = []\n",
    "    cols = imgwidth//width +1\n",
    "    rows = imgheight//height\n",
    "    for i in range(rows):\n",
    "        combined = h_stitch(cropped_list[cols*i:cols*i + (cols)])\n",
    "        combined_list.append(combined)\n",
    "    output = get_concat_v(combined_list)\n",
    "    output = output.crop((0,0,imgwidth, imgheight))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('gan256-2c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploads = FileUpload()\n",
    "uploads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, file_info in uploads.value.items():\n",
    "    img = Image.open(io.BytesIO(file_info['content']))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_image(img, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "lesson7-superres.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
